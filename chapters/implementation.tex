% !TEX encoding = UTF-8 Unicode
%!TEX root = main.tex
% !TEX spellcheck = en-US
%%=========================================

\chapter{Implementation}
\label{ch:implementation}

The library will be written in C, with standard GNU99. FIXME why. POSIX.


FIXME

organization of code, what each code file implements.  

\section{Data Structures}

Some data structures was specified in the design, namely a queue and a tree structure. These data structures are not supported in the C standard library, and has to either be implemented or use an existing third\hyp{}party implementation.

For this project, the BSD libc implementations of queues and trees, \texttt{sys/queue.h} \citep{manqueue} and \texttt{sys/tree.h} \citep{mantree} respectively, is used. \texttt{sys/queue.h} implements four types of queues: singly\hyp{}linked lists, singly\hyp{}linked tail queues, lists, and tail queues. \texttt{sys/tree.h} implements two types of trees: red\hyp{}black trees and splay trees. 

These implementations are header\hyp{}only, dependency free, requires no dynamic allocations, and are type\hyp{}safe. This is highly beneficial for the development of the run\hyp{}time system. No dynamic allocations will also lower the run\hyp{}time overhead. 

Tail\hyp{}queues, hereafter called tailq, and red\hyp{}black trees, hereafter called rb\hyp{}tree, will be used in the scheduler implementation. 

FIXME examples.


\section{Thread\hyp{}specific Data}

TSD is implemented using \texttt{pthread\_key\_t} type, which is a POSIX implementation. The TSD type \texttt{pthread\_key\_t} stores a single variable of type \texttt{void*}. One TSD variable is used in this implementation, which is a pointer to the corresponding scheduler for each kernel\hyp{}thread. With the TSD variable, the internal procedure \texttt{scheduler\_self} returns a pointer the corresponding scheduler struct for the given kernel\hyp{}thread. This is only used internally in the run\hyp{}time system, and is invisible to the programmer. The POSIX method \texttt{pthread\_getspecific()} is used to get the actual TSD variable.

\begin{lstlisting}[style={CustomC},caption={Procedure to find the scheduler for a given kernel\hyp{}trhead}]
pthread_key_t g_key_scheduler;
Scheduler* scheduler_self(void) {
    return pthread_getspecific(g_key_scheduler);
}
\end{lstlisting}

This can also be used to find the current running process struct, as the scheduler always records which process is currently running. It is called \texttt{process\_self}, and is also an internal procedure for the run\hyp{}time system, just as the \texttt{scheduler\_self} procedure.

\begin{lstlisting}[style={CustomC},caption={Procedure to find the current running process}]
Process* process_self(void) {
    return scheduler_self()->running_process;
}
\end{lstlisting}

Since this is a single\hyp{}core implementation, a simple global variable would achieve the same results as a \texttt{pthread\_key\_t} variable. However, it would not work for multi\hyp{}processor support. So \texttt{pthread\_key\_t} is used for a cleaner, scalable and portable implementation.

FIXME overhead.

\section{User\hyp{}threads and Context Switching}

User\hyp{}threads are implemented as coroutines. Each user\hyp{}thread is its own coroutines. There are multiple ways to implement coroutines. Most known approach is using the library routines \texttt{setjmp} and \texttt{longjmp} to provide non\hyp{}local flow between different call frames on the stack. This is however not a portable solution, as jumping down the stack relies on undefined behaviour. The C standard does not specify deallocated stack frames to retain in memory when jumping down the stack, which would break on architectures which frees deallocated stack frames.

Another approach is using a duff's device \citep{duffsdevice}, splitting up a process into atomic instructions between scheduling points. This is both portable and does not rely on undefined behaviour. However, the procedure of splitting up and defining the atomic instructions can be difficult to implement. This has usually in existing libraries been implemented using macro magic, which is not favorable for this implementation.

The third approach is stackful coroutines, which is used in this implementation. This allows for fast context switches between coroutines, as they are usually implemented in a few inline assembly instructions. However, the stack has in most implementations fixed size, which makes stack overflow a potential problem. This is discussed in more detail in Section FIXME. 

Each coroutine has its own stack and a context. The stack acts as the stack frame of the coroutine execution, and is either allocated on the stack (of the main process) or on the heap. The context is a snapshot of the processor registers at a given execution time, and is stored in a C\hyp{}struct. The snapshot consists of registers that must be preserved as of System V ABI calling conventions, and the program counter. Preserved registers differs from calling convention to calling convention, depending on which architecture and operating system is used, which makes portable implementation a challenge.  

A context\hyp{}switch from a coroutine \textit{c1} to a coroutine \textit{c2} does the following: \textit{c1} invokes a context switch from its own context to the context of \textit{c2}. A snapshot of the processors registers are stored in \textit{c1}'s context struct, and the snapshot stored in \textit{c2}'s context struct is loaded into the processors registers. The program counter is the last register to be loaded, as this triggers the continuation of \textit{c2}. From now on, the processors is executing in the coroutine \textit{c2}.

Currently, context switching is only implemented for x86\_64 and i386 platforms. The context struct for i386 and x86\_64 is in Listing \ref{lst:ctx_i386} and \ref{lst:ctx_x86_64}, respectively. Note that each register is stored as 32\hyp{}bit or 64\hyp{}bit variables, as i386 is a 32\hyp{}bit platform and x84\_64 is a 64\hyp{}bit platform, and the registers are of such width.

\noindent\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption={Context struct for i386},style={CustomC},label={lst:ctx_i386}]
struct Context {
    // preserved registers
    uint32_t ebx;  
    uint32_t esi;
    uint32_t edi;
    uint32_t ebp;
    uint32_t esp;
    // program counter
    uint32_t eip;
};
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption={Context struct for x86\_64},style={CustomC},label={lst:ctx_x86_64}]
struct Context {
    // preserved registers
    uint64_t rbx;  
    uint64_t rsp;
    uint64_t rbp;
    uint64_t r12;
    uint64_t r13;
    uint64_t r14;
    uint64_t r15;
    // program counter
    uint64_t rip;
};
\end{lstlisting}
\end{minipage}

The context switching procedure has the following C function prototype.

\begin{lstlisting}[style={CustomC},frame={},numbers=none]
void context_switch(Context *from, Context *to);
\end{lstlisting}

It is however implemented in inline assembly, as direct access to registers is not supported in native C. See Appendix \ref{ch:assembly} for full implementation.

FIXME stack and context initialization.

\section{Yielding}

Yielding is used to transfer the flow of control from a running process back to the scheduler. To achieve this, the context of both the running process and the scheduler must be available. Getting either the scheduler pointer or the process pointer is enough, as the scheduler and the process has a pointer to each other. 

For the internal process, calling the \texttt{scheduler\_self} would be sufficient, however there is a overhead penalty of acquiring the TSD variable. The yielding procedure is a frequently called procedure by the run\hyp{}time system, and the overhead would quickly accumulate. A simple optimization would be passing the process pointer if it is already available for the caller, and if not just pass a \texttt{NULL} pointer and find the process struct through the scheduler pointer. 

See Listing \ref{lst:yielding_procedure} for both internal and external procedure implementation.

\begin{lstlisting}[style={CustomC},caption={Internal and external yielding procedure},label={lst:yielding_procedure}]
// Internal procedure
void process_yield(Process *process) {
    Scheduler *scheduler = NULL;
    if (process == NULL) { // Process struct is not known
        scheduler = scheduler_self();
        process = scheduler->current_process;
    } else { // Process struct is known
        scheduler = process->scheduler;
    }
    context_switch(&process->context, &scheduler->context);
}
// External procedure
void YIELD(void) {
    process_yield(NULL);
}
\end{lstlisting}

Note that the external procedure has to call the yielding procedure with NULL as argument, since the process pointer is not available. The internal process will then find what the process pointer is.

\section{Run\hyp{}time System}

The function call \texttt{START} starts the run\hyp{}time system, and takes a function pointer to the main process. The run\hyp{}time system creates a scheduler, and creates a process with the given function pointer. The main process is then added to the ready queue of the scheduler, as well as registered as the main process in the scheduler. After the initialization, the main loop of the scheduler is invoked. Whenever the scheduler exits the main loop, the scheduler is freed and the \texttt{START} procedure returns. 

\begin{lstlisting}[style={CustomC},caption={\texttt{START} procedure}]
void START(ProcFxn fxn) {
    Scheduler *scheduler;
    scheduler_create(&scheduler);
    Process *process;
    process_create(&process, fxn);
    scheduler->main_process = process;
    scheduler_addready(process);
    // Start the scheduler main loop
    scheduler_run();

    scheduler_free(scheduler);
}
\end{lstlisting}

The function call \texttt{EXIT} signals the scheduler to exit the main loop. The scheduler exit flag i set, and context switches back to the scheduler. This in turn will return to the \texttt{START} procedure, which cleans up the run\hyp{}time system. 

\begin{lstlisting}[style={CustomC},caption={\texttt{EXIT} procedure}]
void EXIT(void) {
    Scheduler *scheduler = scheduler_self();
    scheduler->is_exit = 1;
    process_yield(scheduler->current_process);
}
\end{lstlisting}

\subsection{Scheduler}

The scheduler is realized by a scheduler struct with method functions which operates on a given scheduler struct. The scheduler struct type is shown in Listing \ref{lst:scheduler_struct_type}.

\begin{lstlisting}[style={CustomC},caption={Scheduler struct type},label={lst:scheduler_struct_type}]
struct Scheduler {
    Context context;
    size_t  stack_size;
    size_t  page_size; 
    int     is_exit;
    Process *main_process;
    Process *running_process;
    // queues
    struct ProcessQ totalQ;
    struct ProcessQ readyQ;
    // trees
    struct ProcessRB sleepRB;
    struct PRocessRB altsleepRB;
};
\end{lstlisting}

%Listing \ref{} shows the static and non\hyp{}static methods which operates on a given scheduler struct.
%
%\begin{lstlisting}[style={CustomC},caption={Scheduler struct type},label={lst:scheduler_methods}]
%Scheduler* scheduler_self(void);
%int  scheduler_create(Scheduler **new_scheduler);
%void scheduler_free(Scheduler *scheduler);
%void scheduler_addready(Process *process);
%void scheduler_remready(Process *process);
%void scheduler_addsleep(Process *process);
%void scheduler_remsleep(Process *process);
%void scheduler_addaltsleep(Guard *guard);
%void scheduler_remaltsleep(Guard *guard);
%void _scheduler_wakeup(Scheduler *scheduler);
%void _scheduler_checkQs(Scheduler *sched);
%int  _scheduler_running(Scheduler *sched);
%int  scheduler_run(void);
%\end{lstlisting}

The scheduler constructor is called \texttt{scheduler\_create}. When created, the scheduler struct is allocated on the heap. The default stack size for the coroutines is determined, the page size of the memory management unit is stored, and the exit flag is nulled out. The scheduler struct pointer is then registered in the TSD variable. The context of the scheduler and the two tailqs and rb\hyp{}trees is initialized.

The scheduler destructor is called \texttt{scheduler\_free}. When cleaned up, the total queue is iterated over and calls the cleanup procedure for each process. Lastly, the scheduler struct pointer is freed up. 

The scheduler main loop follows the design in Section \ref{subsec:scheduler}. Some slight simplifications has been made as a result of the implementation being single\hyp{}core. 

When checking the ready queue and no processes are ready, the kernel\hyp{}thread is suspended by sleeping the minimal expiration time in either sleep or alternation sleep rb\hyp{}tree. The wakeup procedure, which checks both sleep rb\hyp{}trees for expired timeouts, are straight forward. However, some slight care has to be taken with the alternation sleep tree. Whenever an sleeping alternation has expired, the scheduler must check if the process has not already been added to the ready queue by another process. The scheduling policy is currently a FIFO queue on the ready queue. 

Registering the current process is done by setting the next process as current process, and setting the state of that process to \textit{Running}. Then a context switch is done to that process. One additional action is done after the running process yields. A procedure checking the stack usage of the process will advise the kernel of memory usage if the stack usage exceeds a certain limit. This is to reduce the total memory footprint of the run\hyp{}time system. See Listing \ref{lst:scheduler_registering_context_switch} for reference.

\begin{lstlisting}[style={CustomC},caption={Scheduler registering and context switch to running process},label={lst:scheduler_registering_context_switch}]
next_process->state = PROCESS_RUNNING;
scheduler->current_process = next_process;
context_switch(&scheduler->context, &next_process->context);
scheduler->current_process = NULL;
memory_advise(next_process->stack);
\end{lstlisting}

The epilogue does the following actions on the resulting state of the yielding process. 

\begin{lstlisting}[style={CustomC},caption={Handling of process state in epilogue},label={lst:process_state_epilogue}]
switch(next_process->state) {
case PROC_RUNNING:
case PROC_READY: // fallthrough
    scheduler_addreadyQ(scheduler, next_process);
    break;
case PROC_ENDED:
    scheduler->is_exit = scheduler->is_exit 
                      || (next_process == scheduler->main_process);
    composite_parse(next_process->composite_block);
    process_free(next_process);
    break;
case PROC_WAIT:
case PROC_SLEEP: // fallthrough
    // Do nothing
    break;
case ERROR:
    process_error(next_process);
    break;
}
\end{lstlisting}

\subsection{Processes}

A process is realized by a process struct with method functions which operates on a given process struct. The process struct type is shown in Listing \ref{lst:process_struct_type}. The scheduler related node members is not directly used by the process struct, but by the data structure methods.

\begin{lstlisting}[style={CustomC},caption={Process struct type},label={lst:process_struct_type}]
typedef void (*ProcessFxn)(void);
struct Process {
    Scheduler *scheduler;
    enum {
        PROC_ERROR = 0,
        PROC_READY,
        PROC_RUNNING,
        PROC_ENDED,
        PROC_SLEEP,
        PROC_WAIT
    } state;
    // fxn and args 
    ProcessFxn fxn;
    struct {
        size_t num;
        void   **ptr; @\label{line:process_args}@
    } args;
    // coroutine related
    Context context;
    struct {
        size_t size;
        size_t used;
        void   *ptr;
    } stack;
    // process sleep metric
    uint64_t sleep_usec;
    // composite process related
    Composite *composite_block; @\label{line:composite_block}@
    // scheduler tailq and rb-tree nodes
    TAILQ_ENTRY(Process) totalQ_node;
    TAILQ_ENTRY(Process) readyQ_node;
    RB_ENTRY(Process) sleepRB_node;
    RB_ENTRY(Process) altsleepRB_node;
};
\end{lstlisting}

The process constructor is called \texttt{process\_create}, and takes a function pointer as an argument. The function pointer is the function the process is to execute. When created, the process struct is allocated on the heap. The stack is allocated and the context is initialized. The function pointer is set, but not the arguments. The state is set to \textit{Ready}, and the scheduler pointer is set to the appointed scheduler through \texttt{scheduler\_self} method. The rest of the struct members, except the tailq and rb\hyp{}tree nodes, are zero\hyp{}initialized. Lastly, the process struct is inserted in the total queue of the scheduler. This gives the ownership of the process to the scheduler. 

The process destructor is called \texttt{process\_free}. The process is removed from the total queue of the scheduler, and the stack and argument array is freed. Lastly, the process struct is freed.

The process execution flow does not start from the specified function pointer. Instead, the process execution starts at an internal library function \texttt{process\_mainfxn}. This procedure appropriately calls the function pointer, and sets the process state to \textit{Ended} and yields when the process function returns. The process main function is shown in Listing \ref{lst:process_main_function}. This approach of abstracting the function call away, at Line \ref{line:function_call} in Listing \ref{lst:process_main_function}, constrains the number of function arguments to a fixed number if the arguments were to be passed directly in the function call.

\begin{lstlisting}[style={CustomC},caption={Process main function},label={lst:process_main_function}]
void process_mainfxn(Process *process) {
    process->fxn(); @\label{line:function_call}@
    process->state = PROC_ENDED;
    process_yield(process);
    // This is never reached
}
\end{lstlisting}

Allowing arbitrary number of function arguments is much more expressive than a fixed number, which is why the arguments are rather stored as an argument array in the process struct, seen at Line \ref{line:process_args} in Listing \ref{lst:process_struct_type}. This argument array can then be accessed through the API function \texttt{ARGN}, see Listing \ref{lst:argn_api_call}. This does however constrain the type of arguments to only pointers. 

\begin{lstlisting}[style={CustomC},caption={\texttt{ARGN} API function},label={lst:argn_api_call}]
void* ARGN(size_t n) {
    Process *process = process_self();
    return (n < process->args.num)
        ? process->args.ptr[n]
        : NULL;
}
\end{lstlisting}

The added flexibility of having arbitrary number of arguments does in my opinion outweigh the added overhead of storing and accessing the arguments through API calls.

Since function arguments are optional, the argument array is populated by a separate meth\-od \texttt{process\_setargs}. It takes a variadic list of void pointers, allocates an argument array, and copies over the pointers from the variadic list to the argument array. Example of how this is used by the run\hyp{}time system when creating a process is shown in Listing \ref{lst:process_setargs_example}.

\begin{lstlisting}[style={CustomC},caption={Process creation example example},label={lst:process_setargs_example}]
void process_setargs(Process *process, va_list args);
// Takes function arguments as a variadic list 
Process* example_create_process(ProcessFxn fxn, ...){
    Process *process;
    process_create(&process, fxn);
    va_list args;
    va_start(args, fxn);
    process_setargs(process, args);
    va_end(args);
    return process;
}
\end{lstlisting}

This is why the process constructor does not allocate and populate the argument array, while the destructor frees the argument array. 

\section{Composite Processes}

Composite processes are implemented as node trees, following the design in Section \ref{sec:composite_processes} with some slight modifications. The challenge is how the composite tree blocks are implemented. One approach is to create a ``\textit{god}'' struct for the block type, containing all data necessary for each block type. Another more modular approach is to create a struct for each composite tree block type, and one generic block type. The generic block is the interface, which his reinterpret casted to the corresponding composite tree block type. The latter approach is used in this implementation.

Five structs are defined: a Header, Block, ProcBlock, ParBlock, and SeqBlock. Go and Run is not used as they are optimized away, explained later. The header struct defines the common interface between the generic block and the typed blocks. Listing \ref{lst:header_struct_type} shows the header struct. The type of the block is set in the header file, the parent of the block, some variables for finding the root of the composite process tree, and a pointer to an optional root process. The node variable is for Block queues in \texttt{PAR} and \texttt{SEQ} blocks.

\begin{lstlisting}[style={CustomC},caption={Header struct type},label={lst:header_struct_type}]
typedef struct {
    enum {
        PROC_BLOCK,
        PAR_BLOCK,
        SEQ_BLOCK
    } type;
    Block *parent;
    int is_root;
    Process *root_process;
    // node tree related
    TAILQ_ENTRY(Block) node;
} Header;
\end{lstlisting}

All structs Block, ProcBlock, ParBlock and SeqBlock must have the header struct as a member variable, and it must be the \underline{\smash{first}} member variable. This allows the composite process procedures and block queues to work on the generic block struct. To determine the block type, inspect the header member. Then reinterpret cast the block pointer to the correct block type through type punning. Listing \ref{lst:block_struct_type},\ref{lst:procblock_struct_type},\ref{lst:parblock_struct_type},\ref{lst:seqblock_struct_type} shows the different block types.

\noindent\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption={Generic Block struct type},style={CustomC},label={lst:block_struct_type}]
typedef struct {
    Header header;
} Block;
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption={\texttt{PROC} Block struct type},style={CustomC},label={lst:procblock_struct_type}]
typedef struct {
    Header header;
    Process *process;
} ProcBlock;
\end{lstlisting}
\end{minipage}
\noindent\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption={\texttt{PAR} Block struct type},style={CustomC},label={lst:parblock_struct_type}]
typedef struct {
    Header header;
    struct {
        size_t num;
        struct BlockQ Q;
    } childs;
} ParBlock;
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption={\texttt{SEQ} Block struct type},style={CustomC},label={lst:seqblock_struct_type}]
typedef struct {
    Header header;
    struct {
        size_t num;
        Block *current;
        struct BlockQ Q;
    } childs;
} SeqBlock;
\end{lstlisting}
\end{minipage}

Type punning is done through union casting. In other words, a pointer is casted to a union of the source and destination type, and reading from the destination type yields the reinterpreted pointer. This is the correct way to achieve type punning without breaking strict aliasing optimizations. A convenience macro can be made to easily achieve type punning, shown in Listing \ref{lst:type_punning}. Note that union casting and \texttt{\_\_typeof\_\_} is a GNU C extension.

\begin{lstlisting}[style={CustomC},caption={Type punning through union cast},label={lst:type_punning}]
#define BLOCK_CAST(block, destType) \
    (((union {__typeof__(block) src; destType dst;})block).dst)
\end{lstlisting}

Listing \ref{lst:type_punning_example} shows how type punning is used to convert from a generic block struct to the given typed block, and back from typed block to generic block type. This is used any time when the run\hyp{}time system either goes either way between a generic block to a typed block. 

\begin{lstlisting}[style={CustomC},caption={Type punning example},label={lst:type_punning_example}]
Block* type_punning_example(Block *block) {
    switch (block->type) {
    case PROC_BLOC: {
        ProcBlock *proc_block = BLOCK_CAST(block, ProcBlock*);
        // ... work ...
        return BLOCK_CAST(proc_block, Block*);
    }
    case PAR_BLOCK: {
        ParBlock *par_block = BLOCK_CAST(block, ParBlock*);
        // ... work ...
        return BLOCK_CAST(par_block, Block*);
    }
    case SEQ_BLOCK: {
        ProcBlock *seq_block = BLOCK_CAST(block, SeqBlock*);
        // ... work ...
        return BLOCK_CAST(seq_block, Block*);
    }
}
\end{lstlisting}

Defining a composite tree process is done through the API calls \texttt{PROC}, \texttt{PAR}, \texttt{SEQ}, \texttt{GO}, and \texttt{RUN}. The function prototypes for the API calls are shown in Listing \ref{lst:composite_process_api_prototypes}. Both \texttt{PAR} and \texttt{SEQ} has a single child block as first argument, because a non\hyp{}variadic argument must be specified before the variadic list. Also, if the variadic list is empty, the given block can be optimized away as a result of the redundancy identity shown in Figure \ref{fig:composite_axiom}.

\begin{lstlisting}[style={CustomC},caption={Composite process API prototypes},label={lst:composite_process_api_prototypes}]
Block* PROC(ProcessFxn fxn, ...);
Block* PAR(Block *first_child, ...);
Block* SEQ(Block *first_child, ...);
void GO(Block *root);
void RUN(Block *root);
\end{lstlisting}

Currently, the API calls \texttt{PROC}, \texttt{PAR} and \texttt{SEQ} allocates a typed block on the heap. \texttt{GO} or \texttt{RUN} does not however allocate any typed block on the heap. \texttt{PROC} also creates a process with the given function pointer and arguments, and sets the process pointer in the typed block to the created process. \texttt{PAR} and \texttt{SEQ}, if two or more childs given, inserts each child in the block queue and records the number of childs. \texttt{SEQ} initializes the current block pointer to the first child. 

Both API calls \texttt{GO} and \texttt{RUN} receives a root block, which the root flag of the block is set to true. \texttt{GO} traverses the resulting composite process tree, starting at the root block. When traversing returns, the API call returns as well, and the running process continues execution. \texttt{RUN} sets the root process pointer to the running process. Then, traverses the resulting composite process tree, starting at the root block. When traverses returns, the running process yields with the process state set to \textit{Wait}. The running process is resumed when the root block finishes.

The traversal implementation is very straightforward, following the pseudocode in Listing \ref{lst:algo_composite_process_tree_traversal}. Only difference is process creation happens in the API call \texttt{PROC}, rather in the traversal procedure. This has to do with the simplifications of the variadic list handling. 

The \texttt{PROC} ended implementation is follows the pseudocode in Listing \ref{lst:algo_composite_process_tree_proc_ended} with some slight modifications. The \texttt{RUN} and \texttt{GO} cases are removed, as they are simplified to a root flag. Whenever either \texttt{PROC}, \texttt{SEQ} or \texttt{PAR} block, a \texttt{block\_done} variable is set to true. This is checked at the end of the switch case, and if the current block is the root block, the root process is rescheduled if the composite tree is synchronous. 

An added cleanup procedure must be called when the root block finishes, as the entire tree is allocated on the heap. This is a simple tree traversal, freeing up each block in a post\hyp{}order fashion. 

\section{Channels}

Channel implementation follows the design in \ref{sec:channels}, implementing a synchronous, unbuffered, size\hyp{}safe channels. However, disjointed channels proved to be to difficult to implement, which is why only any\hyp{}to\hyp{}any channels are used. This is explained more later. 

Channels is realized by a channel struct and a channel end struct. A channel struct represents a created channel, while a channel end is used by a process waiting on a channel. The two struct are shown in Listing \ref{lst:channel_type_struct} and \ref{lst:channel_end_type_struct}.

\noindent\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption={Channel type struct},style={CustomC},label={lst:channel_type_struct}]
typedef struct {
    size_t data_size;
    struct ChanEndQ endQ;
    struct ChanEndQ altQ;
} Chan;
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption={Channel end type struct},style={CustomC},label={lst:channel_end_type_struct}]
typedef struct {
    enum {
        CHAN_WRITER,
        CHAN_READER,
        CHAN_ALTER,
    } type;
    void *data;
    Chan *chan;
    Process *process;
    AltGuard *guard;
    TAILQ_ENTRY(ChanEnd) node;
} ChanEnd;
\end{lstlisting}
\end{minipage}

The channel constructor is called \texttt{chan\_create}, and takes the size of the data type as an argument. The channel struct is allocated on the heap. The data size is stored, and the two channel end queues are initialized. The API call \texttt{CHOPEN} calls the channel constructor and returns the channel struct pointer. 

The channel destructor is called \texttt{chan\_free}, and frees the channel struct. The API call \texttt{CHCLOSE} calls the channel destructor.

The channel struct contains two channel end queues, called hereafter end queue and altend queue. These queues are used when processes has to wait on a channel for a process to be available on the other end. Two queues are used instead of one, as alternation waiting is separated from normal waiting. The end queue can contain both writers and readers, but not at the same time. The altend queue only contains alternation processes trying to read the channel. 

Three operations can be done on a channel struct: channel write, channel read and channel alternation read. All these three operations take a channel struct pointer, a data pointer, and the size of the data as arguments. As well are two additional internal operations for alternation, namely alternation enabling and disabling of a channel. 

A channel read does the following: the channel end queue of the channel is checked. If the queue is non\hyp{}empty and contains writers, the first channel end is popped from the end queue. The data transfer is completed, and the writer process is rescheduled. The reading process resumes execution. If the end queue is empty or contains readers, a channel end struct is allocated on the stack and initialized. The channel end is inserted at the end of the end queue of the channel, and the reader process yields with the state \textit{Wait}. When the process is resumed, the channel operation is already completed. The API call \texttt{CHREAD} calls the channel read operation, and returns whether the write was successful or not. 

A channel write does the following: before the altend queue is checked before the end queue. If it contains an alternation process, the writer tries to accept the alternation event. If it is accepted, the data transfer is completed, and the alternation process is rescheduled. If this fails, the end queue is checked. If it is non\hyp{}empty and contains readers, the first channel end is popped from the end queue. The data transfer is completed, and the reader process is rescheduled. The writer process resumes execution. If the end queue is empty or contains writers, a channel end struct is allocated on the stack and initialized. The channel end is inserted at the end of the end queue of the channel, and the reader process yields with the state \textit{Wait}. When the process is resumed, the channel operation is already completed. The API call \texttt{CHWRITE} calls the channel write operation, and returns whether the write was successful or not. 

A channel alternation read consists of multiple stages. First, the alternation process enables the channel for alternation synchronization. The end queue is checked, and if it contains writers, the channel is marked as available. If empty or contains readers, the alternation process is inserted in the altend queue. The case where the channel is available, the alternation process completes the data transfer with the channel alternation read operation. This pops the first writer from the end queue, the data transfer is completed, and the waiting writer process is rescheduled. Lastly, regardless if the alternation process or a writer process completed the channel operation, the alternation process disables the channel for alternation synchronization. This removes the channel end from the altend queue. The channel alternation read operation is done internally by the run\hyp{}time system.

Transfer of data is optimized depending on the data size. If the data size is either 1, 2, 4, or 8 bytes, data is transferred as integer assignments. If data size is zero, nothing happens. Every other data size is transferred with a normal \texttt{memcpy} call. See Listing \ref{lst:channel_data_transfer} on how this is done.

\begin{lstlisting}[caption={Channel data transfer},style={CustomC},label={lst:channel_data_transfer}]
void channel_datatransfer(void *dst, void *src, size_t size) {
    switch (size) {
    case 0:    /* do nothing */                   break;
    case 1:   *(uint8_t *)dst =  *(uint8_t *)src; break;
    case 2:  *(uint16_t *)dst = *(uint16_t *)src; break;
    case 4:  *(uint32_t *)dst = *(uint32_t *)src; break;
    case 8:  *(uint64_t *)dst = *(uint64_t *)src; break;
    default: memcpy(dst, src, size); break;
    }
}
\end{lstlisting}

This implementation only implements an any\hyp{}to\hyp{}any channel. Implementing one\hyp{}to\hyp{}one, any\hyp{}to\hyp{}one, or one\hyp{}to\hyp{}any channels proved to be too difficult. There was no possibility of enforcing this during compile\hyp{}time, as C lacks the language features for this. Run\hyp{}time checks was possible, but created too much overhead and was too error\hyp{}prone to be any useful. So for the sake of code simplicity, only implementing an any\hyp{}to\hyp{}any channel was favored. 

\section{Alternation}

Alternation is realized by an alternation struct for the alternation process, and a guard struct for each guarded event. Listing \ref{lst:alt_type_struct} and \ref{lst:guard_type_struct} shows the struct definition for the alternation type and guard type, respectively. 

The alternation constructor is called \texttt{alternation\_init}, and takes a pre\hyp{}allocated alternation struct as argument. The alternation struct is allocated on the stack. The constructor initializes the struct members, guard queue and saves the running process pointer. This is used to reschedule the alternation process if it suspends itself waiting. 

The alternation destructor is called \texttt{alternation\_cleanup}, and takes an alternation struct as argument. It iterates over the guard queue, calling the guard destructor for each guard. The available guard array is freed as well, as it is allocated on the heap. 

\noindent\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption={Alternation type struct},style={CustomC},label={lst:alt_type_struct}]
typedef struct {
    int key_count;
    int is_accepted;
    Guard *winner;
    struct {
        int num;
        Guard **guards;
    } ready;
    struct {
        size_t num;
        struct GuardQ Q;
    } guards;
    // skip and time guards 
    // only exists one of
    Guard *guard_skip; 
    Guard *guard_time;
    // alternation process
    Process *process;
} Alternation;
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption={Guard type struct},style={CustomC},label={lst:guard_type_struct}]
typedef struct {
    enum {
        GUARD_SKIP,
        GUARD_TIME,
        GUARD_CHAN
    } type;
    int key;
    Alternation *alternation;
    TAILQ_ENTRY(Guard) Qnode;
    RB_ENTRY(Guard) RBnode;
    // Timer Guard
    uint64_t  usec;
    // Chan Guard
    Chan *chan;
    ChanEnd ch_end;
    struct {
        void *ptr;
        size_t size;
    } data;
    int in_chan;
} Guard;
\end{lstlisting}
\end{minipage}

The guard constructor is called \texttt{alternation\_guardcreate}, and is used for all of the three types of guards. The guard struct is allocated on the heap. It takes a guard type, expiration time in microseconds, channel struct pointer, data pointer, and the size of the data. The API call \texttt{CHAN\_GUARD} calls the constructor with the \texttt{GUARD\_CHAN} type and the channel related arguments. Note that the channel end struct is allocated alongside the guard struct. The API call \texttt{TIME\_GUARD} calls the constructor with the \texttt{GUARD\_TIME} type and the expiration time. Lastly, the API call \texttt{SKIP\_GUARD} calls the constructor with only the \texttt{GUARD\_SKIP} type. 

The guard destructor is called \texttt{alternation\_guardfree}, and frees the guard struct.

The alternation process stages follows the design in Section \ref{subsec:alternation_process_stages}. It starts with defining different guards with the following API calls \texttt{SKIP\_GUARD}, \texttt{TIME\_GUARD} and \texttt{CHAN\_GUARD}. Each call evaluates the boolean condition supplied as argument. If true, calls the corresponding guard constructor and returns the guard struct pointer. If false, a \texttt{NULL} pointer is returned. 

The list of guards is stored as a variadic lsit and sent to as arguments to the API call \texttt{ALT}. The alternation struct is allocated on the stack, and initialized with the alternation constructor. Then, the variadic list is iterated over, adding each guard to the alternation struct. A \texttt{NULL} pointer guard is skipped, but the key value is incremented regardless. After evaluating and adding each guard, the selection procedure is called. Following the event selection algorithm in Section \ref{subsec:event_selection}, some slight additions are used. The available guards array is allocated as an guard struct pointer array, equal to the number of enabled guards. This is dynamically allocated on the heap. The chosen guard is stored in the \texttt{winner} member in the alternation struct. For the uniform pseduo\hyp{}random function, the library function \texttt{rand} is used.

\section{Timers}

Timers is realized through the sleep tree in the scheduler. Invoking a timer on a process is done through the API call \texttt{SLEEP}, which takes an expiration time in microseconds as argument. If the expiration time is zero, the process yields with no additional changes. If non\hyp{}zero, the process registers the expiration time in the process struct, and adds itself to the sleep tree in the scheduler. Then, yielding with the state \textit{Sleep}. When the time expires, the scheduler will reschedule the sleeping process.

