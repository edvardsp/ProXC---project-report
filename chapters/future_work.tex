% !TEX encoding = UTF-8 Unicode
%!TEX root = main.tex
% !TEX spellcheck = en-US
%%=========================================

\chapter{Future Work}
\label{ch:future_work}

Some optional features that was planned for the library were not implemented because time proved too short. In this chapter these features are discussed, and how the library can benefit from this. 

\section{Multicore support}

The most prominent feature that could provide great increase in performance and flexibility would be multi\hyp{}core support. As \citet{c++csp2} argues, taking advantage of the potential parallelism in multicore is the future of concurrency programming.

Multi\hyp{}core support was planned at the start of this project. But, as a result of the overwhelming amount of work, a single\hyp{}core implementation was first planned to make the foundation of the library. Later, when the foundation was set, the plan was to expand to multi\hyp{}core support. However, when the single\hyp{}core implementation was finished, there was not enough time left for the multi\hyp{}core support to be implemented.

The design for the multi\hyp{}core support was however made. In short terms, the library spawns a kernel\hyp{}thread for each online processor core. Each core runs its own scheduler, with its own set of processes to schedule. Spawned processes are distributed through distributed work stealing among schedulers. All access by the scheduler and run\hyp{}time system to structures that is subjected to race conditions is guarded by atomic constructs. The multi\hyp{}core algorithms for the channel and alternation constructs is a modified version of those described in \citet{c++csp2}.


Interfacing blocking calls, such as IO operations or system calls, will cause the entire kernel\hyp{}thread to block. It is possible to mitigate this by detaching the blocking user\hyp{}thread to its own kernel\hyp{}thread, and continue operating on the other kernel\hyp{}thread. When the blocking user\hyp{}thread resumes, it is added back to the scheduler.

Implementing should be trivial to some degree, as this involves adding extra logic for detaching and attaching user\hyp{}threads to schedulers. However, further testing is required to test for correctness.

\section{Replicators}

Replicators is used in occam and XC to specify ranges of components for both \texttt{SEQ}, \texttt{PAR} and \texttt{ALT} constructs. This allows the programmer to specify finite ranges of processes in composite processes, and ranges of alternatives in alternation constructs. 

\begin{lstlisting}[style={CustomC},frame={},numbers={none}]
SEQ i = 0 FOR n
PAR i = 0 FOR n
ALT i = 0 FOR n
\end{lstlisting}

This is a very expressive and powerful semantic, which is very useful for reusing code. Note that a replicator behaves like a for\hyp{}loop, specifying a range which the composite process or alternation ranges over to generate each process or alternative. 

ProXC does not support replicators, but there is in my opinion no reason not to. Implementing replicators should not be very difficult. An appropriate API must be designed, and further implement the logic where the run\hyp{}time system iteratively generates the processes and alternatives.

\section{More Complex Scheduler Policy}

Currently, a simple FIFO queue is used as the scheduler policy for ready processes. For this simple framework, this policy is good enough. However, if in the future more complex features such as replicators, multi\hyp{}core support, block calls and etc. is implemented, a more complex scheduler policy might be necessary. 

Another factor is how deterministic do we want the library to be. As of now, determinism is not the main focus. But if this changes, a more complex scheduler policy might be needed. 

\section{Stack Reusability, Flexibility and Safety}

Stack usage among processes is the most resource heavy component in the run\hyp{}time system. A single stack is allocated each time a process is spawned, and deallocated when the process has ended. For many cheap and short lived processes, this procedure of allocating and deallocating stacks each time is wasteful. A smarter system of caching already allocated stacks and reusing them for other processes would better utilize stack allocations.

Additionally, the stacks are fixed size. The majority of processes, stack usage is insignificant. However, for processes that require extensive use of the stack, stack overflow is a real issue. A growable stack is possible by having a buffer zone above the stack. When the run\hyp{}time system detects the stack pointer of the process has accessed the buffer zone, the stack is reallocated to a much larger stack size. Finding the best initial fixed and resizeable stack size would need experimenting on different values.

Lastly, safety of stack overflow should also be considered important to implement. A stack overflow in the current system has no guarantees to be detected or cause an error. For all practical purposes, it is undefined behaviour. This could be implemented by having a small region above the buffer zone, which was mentioned above. This buffer zone would be marked as a memory protected area. When a process accesses the buffer zone, a stack overflow has occurred. The kernel will generate a segmentation fault, crashing the program. This does require kernel support.
